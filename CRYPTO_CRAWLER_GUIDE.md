# üöÄ H∆∞·ªõng D·∫´n S·ª≠ D·ª•ng Cryptocurrency Data Pipeline

## Integration v·ªõi Apache Airflow & Big Data Stack

## üì¶ C√†i ƒê·∫∑t

### B∆∞·ªõc 1: C√†i ƒë·∫∑t Python packages

```bash
cd "/Users/nguyentiendang0106/Documents/20251/Big data"
pip install -r requirements.txt
```

Ho·∫∑c c√†i t·ª´ng package:

```bash
pip install requests pandas numpy schedule openpyxl apache-airflow pyspark kafka-python
```

### B∆∞·ªõc 2: C√†i ƒë·∫∑t Big Data Stack (Optional - For Production)

```bash
# Airflow
pip install apache-airflow apache-airflow-providers-apache-spark

# Kafka Python Client
pip install kafka-python confluent-kafka

# Spark
# Download t·ª´ https://spark.apache.org/downloads.html
```

## üéØ Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   DATA PIPELINE FLOW                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    Crawlers (Python)
         ‚îÇ
         ‚îú‚îÄ‚îÄ test.py (Basic)
         ‚îú‚îÄ‚îÄ crypto_crawler_advanced.py (Advanced)
         ‚îÇ
         ‚ñº
    Apache Kafka Topics
         ‚îÇ
         ‚îú‚îÄ‚îÄ crypto.trades.raw
         ‚îú‚îÄ‚îÄ crypto.ohlcv.1m
         ‚îú‚îÄ‚îÄ crypto.market.data
         ‚îÇ
         ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Apache Airflow DAGs          ‚îÇ
    ‚îÇ   (Orchestration)              ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ‚ñ∫ Batch Processing (Spark)
         ‚îÇ   - Daily aggregations
         ‚îÇ   - Feature engineering
         ‚îÇ   - ML model training
         ‚îÇ
         ‚îú‚îÄ‚ñ∫ Stream Processing (Spark)
         ‚îÇ   - Real-time analytics
         ‚îÇ   - Anomaly detection
         ‚îÇ   - Live predictions
         ‚îÇ
         ‚ñº
    Storage Layer
         ‚îÇ
         ‚îú‚îÄ‚îÄ HDFS (Batch data)
         ‚îú‚îÄ‚îÄ HBase (Batch views)
         ‚îî‚îÄ‚îÄ Redis (Real-time views)
         ‚îÇ
         ‚ñº
    Serving Layer (API + Dashboard)
```

## üß™ Testing Strategy

### Unit Tests

```bash
# Test individual crawlers
pytest tests/test_crawlers.py

# Test Airflow DAGs
pytest tests/test_dags.py

# Test Kafka integration
pytest tests/test_kafka_integration.py
```

### Integration Tests

```bash
# Test full ETL pipeline
airflow dags test crypto_etl_dag 2025-11-10

# Test specific task
airflow tasks test crypto_etl_dag crawl_coingecko 2025-11-10
```

### Load Testing

```bash
# Simulate high throughput
python scripts/load_test_kafka.py --rate 10000 --duration 300
```

---

## üìà Monitoring & Observability

### Metrics to Track

**Crawler Metrics:**

- Requests per second
- Success rate
- Response time
- Error rate

**Airflow Metrics:**

- DAG success rate
- Task duration
- Queue size
- Worker utilization

**Kafka Metrics:**

- Message throughput
- Consumer lag
- Partition distribution
- Disk usage

**Data Quality Metrics:**

- Missing values %
- Duplicate records
- Schema violations
- Freshness (time since last update)

### Monitoring Tools

```bash
# Airflow metrics
http://localhost:8080/health

# Prometheus metrics endpoint
http://localhost:9090

# Grafana dashboards
http://localhost:3000
```

---

## üö® Troubleshooting

### Common Issues

#### 1. Airflow DAG not appearing

```bash
# Check DAG file syntax
python airflow/dags/crypto_etl_dag.py

# Refresh DAGs
airflow dags list-import-errors
```

#### 2. Kafka connection failed

```bash
# Check Kafka status
kafka-topics.sh --bootstrap-server localhost:9092 --list

# Test connectivity
telnet localhost 9092
```

#### 3. Rate limit exceeded

```python
# Add exponential backoff
from tenacity import retry, wait_exponential

@retry(wait=wait_exponential(multiplier=1, min=4, max=60))
def crawl_with_retry():
    return crawler.get_data()
```

#### 4. Memory issues in Spark

```python
# Increase executor memory
spark.conf.set("spark.executor.memory", "8g")
spark.conf.set("spark.driver.memory", "4g")

# Repartition data
df = df.repartition(200)
```

#### 5. Airflow task stuck in running

```bash
# Kill zombie tasks
airflow tasks clear crypto_etl_dag --task-regex 'crawl.*' --start-date 2025-11-10

# Restart scheduler
pkill -f "airflow scheduler"
airflow scheduler &
```

---

## üìä Performance Optimization

### Crawler Optimization

```python
# Use async/await for concurrent requests
import asyncio
import aiohttp

async def crawl_async(symbols):
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_price(session, symbol) for symbol in symbols]
        return await asyncio.gather(*tasks)

# Result: 10x faster than sequential
```

### Airflow Optimization

```python
# Enable parallelism
AIRFLOW__CORE__PARALLELISM = 32
AIRFLOW__CORE__DAG_CONCURRENCY = 16
AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG = 3

# Use connection pooling
AIRFLOW__CORE__SQL_ALCHEMY_POOL_SIZE = 10
```

### Kafka Optimization

```yaml
# Producer config
batch.size: 32768
linger.ms: 10
compression.type: lz4

# Consumer config
fetch.min.bytes: 1024
fetch.max.wait.ms: 500
```

---

## ÔøΩ Best Practices

### 1. Idempotent DAGs

```python
# DAGs should be rerunnable without side effects
# Use upsert instead of insert
# Check if data already exists before processing
```

### 2. Data Versioning

```python
# Include version in file names
output_path = f"hdfs:///crypto/ohlcv/v1/{date}/"

# Track schema versions
schema_version = "1.0.0"
```

### 3. Error Handling

```python
# Always log errors with context
try:
    result = crawler.get_data()
except Exception as e:
    logger.error(f"Failed to crawl {symbol}: {e}", exc_info=True)
    send_alert(f"Crawler failed: {symbol}")
    raise
```

### 4. Resource Management

```python
# Always close connections
try:
    producer = KafkaProducer(...)
    # Use producer
finally:
    producer.flush()
    producer.close()
```

### 5. Configuration Management

```python
# Use environment variables
KAFKA_BROKERS = os.getenv('KAFKA_BROKERS', 'localhost:9092')

# Never hardcode credentials
API_KEY = os.getenv('COINGECKO_API_KEY')
```

---

## üîê Security Considerations

### API Keys

```bash
# Use Airflow Connections for secrets
airflow connections add 'coingecko_api' \
    --conn-type 'http' \
    --conn-password 'your_api_key'

# Access in DAG
from airflow.hooks.base import BaseHook
conn = BaseHook.get_connection('coingecko_api')
api_key = conn.password
```

### Network Security

```yaml
# Use TLS for Kafka
security.protocol: SSL
ssl.truststore.location: /path/to/truststore.jks
```

### Data Privacy

```python
# Anonymize sensitive data
df = df.withColumn('user_id', hash_udf('user_id'))
```

---

## üìö Additional Resources

### Documentation

- [Apache Airflow Docs](https://airflow.apache.org/docs/)
- [Kafka Python Client](https://kafka-python.readthedocs.io/)
- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)

### Monitoring Dashboards

- Airflow UI: http://localhost:8080
- Kafka Manager: http://localhost:9000
- Grafana: http://localhost:3000

### Sample Queries

```python
# Query from HBase
from happybase import Connection

conn = Connection('localhost')
table = conn.table('crypto_ohlcv')

# Scan recent data
for key, data in table.scan(row_prefix=b'BTC_'):
    print(key, data)
```

---

## üìù Changelog

### Version 2.0.0 (2025-11-10)

- ‚úÖ Added Airflow integration
- ‚úÖ Added Kafka streaming support
- ‚úÖ Added 3 production DAGs
- ‚úÖ Enhanced monitoring and logging
- ‚úÖ Added comprehensive testing guide

### Version 1.0.0 (2025-10-20)

- ‚úÖ Initial release
- ‚úÖ Basic and advanced crawlers
- ‚úÖ Standalone mode support

---

**Ready to Deploy! üöÄ**

Start with standalone mode for development, then migrate to Airflow orchestration for production.

### Mode 1: Standalone Mode (Development)

#### File 1: `test.py` - Basic Crawler

Crawler c∆° b·∫£n v·ªõi CoinGecko API cho development v√† testing:

```bash
cd data
python test.py
```

**T√≠nh nƒÉng:**

- ‚úÖ Crawl top 50 cryptocurrencies
- ‚úÖ Crawl trending coins
- ‚úÖ Crawl d·ªØ li·ªáu l·ªãch s·ª≠ 30 ng√†y (Bitcoin, Ethereum)
- ‚úÖ Crawl global market data
- ‚úÖ Crawl th√¥ng tin chi ti·∫øt Bitcoin

**Output:** T·∫°o th∆∞ m·ª•c `crypto_data/` v·ªõi c√°c file CSV v√† JSON

**Use Case:** Quick testing, data exploration, prototyping

---

#### File 2: `crypto_crawler_advanced.py` - Advanced Crawler

Crawler n√¢ng cao v·ªõi nhi·ªÅu ngu·ªìn d·ªØ li·ªáu:

```bash
cd data
python crypto_crawler_advanced.py
```

**Menu ch·ª©c nƒÉng:**

```
1. Crawl Top 100 Cryptocurrencies (CoinGecko)
2. Crawl DeFi Protocols
3. Crawl Binance 24h Ticker
4. Crawl Bitcoin Candlestick + Technical Indicators
5. Crawl Bitcoin Orderbook
6. Crawl t·∫•t c·∫£ (One-time)
7. Ch·∫°y Scheduled Crawler (Auto crawl m·ªói 60 ph√∫t)
```

**T√≠nh nƒÉng n√¢ng cao:**

- ‚úÖ Crawl t·ª´ CoinGecko v√† Binance
- ‚úÖ T√≠nh to√°n technical indicators (SMA, EMA, MACD, RSI, Bollinger Bands)
- ‚úÖ Ph√¢n t√≠ch market sentiment
- ‚úÖ Crawl orderbook depth
- ‚úÖ Scheduled tasks (t·ª± ƒë·ªông crawl ƒë·ªãnh k·ª≥)
- ‚úÖ Logging system

**Use Case:** Production-ready crawler, continuous data collection

---

### Mode 2: Airflow Orchestration Mode (Production)

#### Setup Airflow

```bash
# Initialize Airflow database
export AIRFLOW_HOME=~/airflow
airflow db init

# Create admin user
airflow users create \
    --username admin \
    --password admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com

# Start Airflow webserver
airflow webserver --port 8080 &

# Start Airflow scheduler
airflow scheduler &
```

#### DAG 1: `crypto_etl_dag.py` - Daily ETL Pipeline

**Purpose:** Automated daily data collection and processing

**Schedule:** Runs daily at 2:00 AM

**Tasks Flow:**

```python
start ‚Üí crawl_coingecko ‚Üí crawl_binance ‚Üí
validate_data ‚Üí publish_to_kafka ‚Üí
load_to_hdfs ‚Üí trigger_spark_batch ‚Üí
save_to_hbase ‚Üí update_dashboard ‚Üí end
```

**Manual Trigger:**

```bash
# Trigger DAG manually
airflow dags trigger crypto_etl_dag

# Check DAG status
airflow dags list

# View task logs
airflow tasks logs crypto_etl_dag crawl_coingecko 2025-11-10
```

**Configuration:**

```python
# airflow/dags/crypto_etl_dag.py
default_args = {
    'owner': 'crypto_team',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'crypto_etl_dag',
    default_args=default_args,
    description='Daily cryptocurrency ETL pipeline',
    schedule_interval='0 2 * * *',  # Daily at 2 AM
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['crypto', 'etl', 'daily'],
)
```

---

#### DAG 2: `ml_pipeline_dag.py` - ML Training Pipeline

**Purpose:** Weekly model training and evaluation

**Schedule:** Runs every Sunday at 3:00 AM

**Tasks Flow:**

```python
start ‚Üí extract_features ‚Üí
feature_engineering ‚Üí train_price_predictor ‚Üí
train_anomaly_detector ‚Üí evaluate_models ‚Üí
deploy_best_model ‚Üí update_model_registry ‚Üí
send_report ‚Üí end
```

**Manual Trigger:**

```bash
airflow dags trigger ml_pipeline_dag

# Backfill for specific date range
airflow dags backfill ml_pipeline_dag \
    --start-date 2025-10-01 \
    --end-date 2025-10-31
```

**Configuration:**

```python
dag = DAG(
    'ml_pipeline_dag',
    default_args=default_args,
    description='Weekly ML model training and deployment',
    schedule_interval='0 3 * * 0',  # Weekly on Sunday at 3 AM
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['crypto', 'ml', 'training'],
)
```

---

#### DAG 3: `reporting_dag.py` - Analytics & Reporting

**Purpose:** Generate daily reports and metrics

**Schedule:** Runs daily at 8:00 AM

**Tasks Flow:**

```python
start ‚Üí aggregate_daily_stats ‚Üí
calculate_portfolio_metrics ‚Üí
compute_risk_metrics ‚Üí check_alerts ‚Üí
generate_report ‚Üí send_email ‚Üí
update_dashboard_db ‚Üí end
```

**Manual Trigger:**

```bash
airflow dags trigger reporting_dag
```

---

### Mode 3: Kafka Integration Mode (Real-time)

#### Kafka Producer Integration

```python
"""
Integrate crawler with Kafka for real-time streaming
"""
from kafka import KafkaProducer
import json

# Initialize Kafka producer
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# Send data to Kafka topic
def send_to_kafka(data, topic='crypto.ohlcv.1m'):
    try:
        future = producer.send(topic, value=data)
        record_metadata = future.get(timeout=10)
        print(f"Sent to {record_metadata.topic} partition {record_metadata.partition}")
    except Exception as e:
        print(f"Error sending to Kafka: {e}")

# Example: Crawl and stream to Kafka
from crypto_crawler_advanced import AdvancedCryptoCrawler

crawler = AdvancedCryptoCrawler()
df = crawler.crawl_binance_klines('BTCUSDT', interval='1m', limit=1)

# Convert to dict and send
for row in df.to_dict('records'):
    send_to_kafka(row, topic='crypto.ohlcv.1m')
```

#### Run Continuous Streaming

```bash
# Run crawler with Kafka output
python scripts/kafka_streaming_producer.py --symbols BTC,ETH,BNB --interval 1m
```

## üìä D·ªØ Li·ªáu Thu Th·∫≠p ƒê∆∞·ª£c

### CoinGecko Data

- **Market Data:** Gi√° hi·ªán t·∫°i, market cap, volume, price changes
- **Historical Data:** Gi√° l·ªãch s·ª≠ theo ng√†y/gi·ªù
- **Trending Coins:** Top trending cryptocurrencies
- **Global Data:** T·ªïng market cap, volume to√†n th·ªã tr∆∞·ªùng
- **Coin Details:** Th√¥ng tin chi ti·∫øt v·ªÅ t·ª´ng coin

### Binance Data

- **24h Ticker:** Th·ªëng k√™ 24h c·ªßa t·∫•t c·∫£ trading pairs
- **Candlestick (Klines):** OHLCV data v·ªõi nhi·ªÅu timeframes
- **Orderbook:** Bid/Ask depth data
- **Technical Indicators:** SMA, EMA, MACD, RSI, Bollinger Bands

## ÔøΩ Airflow Web UI

### Access Dashboard

```bash
# Open browser
http://localhost:8080

# Login credentials
Username: admin
Password: admin
```

### Monitor DAGs

1. **DAGs View**: See all DAGs and their schedules
2. **Graph View**: Visualize task dependencies
3. **Gantt View**: Analyze task execution timeline
4. **Tree View**: Historical runs overview
5. **Logs View**: Debug task failures

### Key Metrics to Monitor

- ‚úÖ DAG success rate
- ‚úÖ Task duration
- ‚úÖ Data volume processed
- ‚úÖ Retry attempts
- ‚úÖ Failed tasks

---

## üîÑ Complete Workflow Example

### Scenario: Daily Production Pipeline

**Time: 2:00 AM** - Airflow triggers `crypto_etl_dag`

```
1. Task: crawl_coingecko
   - Crawls top 100 coins
   - Saves to temp storage
   Duration: 5 minutes

2. Task: crawl_binance
   - Crawls OHLCV data for 100 symbols
   - Saves to temp storage
   Duration: 10 minutes

3. Task: validate_data
   - Schema validation
   - Data quality checks
   - Remove duplicates
   Duration: 2 minutes

4. Task: publish_to_kafka
   - Send validated data to Kafka topics
   - crypto.ohlcv.1m
   - crypto.market.data
   Duration: 3 minutes

5. Task: load_to_hdfs
   - Batch load from Kafka to HDFS
   - Partition by date and symbol
   Duration: 15 minutes

6. Task: trigger_spark_batch
   - Submit Spark job for aggregations
   - Calculate technical indicators
   - Feature engineering
   Duration: 30 minutes

7. Task: save_to_hbase
   - Save batch views to HBase
   Duration: 10 minutes

8. Task: update_dashboard
   - Refresh dashboard metrics
   Duration: 2 minutes

Total Pipeline Duration: ~77 minutes
```

**Time: 8:00 AM** - Airflow triggers `reporting_dag`

```
Generate daily reports with fresh data
Send email alerts if anomalies detected
```

**Time: 3:00 AM (Sunday)** - Airflow triggers `ml_pipeline_dag`

```
Train ML models with past week's data
Deploy new models if performance improved
```

---

## üõ†Ô∏è Advanced Airflow Features

### 1. Dynamic DAG Generation

```python
# Generate DAGs for multiple symbols
SYMBOLS = ['BTC', 'ETH', 'BNB', 'ADA', 'SOL']

for symbol in SYMBOLS:
    dag_id = f'crypto_etl_{symbol.lower()}'

    dag = DAG(
        dag_id,
        default_args=default_args,
        schedule_interval='*/15 * * * *',  # Every 15 minutes
        tags=['crypto', symbol]
    )

    # Define tasks...
    globals()[dag_id] = dag
```

### 2. XComs for Data Passing

```python
# Task 1: Crawl data and push to XCom
def crawl_data(**context):
    data = crawler.get_top_cryptocurrencies(100)
    context['task_instance'].xcom_push(key='crypto_data', value=data)

# Task 2: Pull from XCom and process
def process_data(**context):
    data = context['task_instance'].xcom_pull(key='crypto_data')
    # Process data...
```

### 3. Sensors for External Dependencies

```python
from airflow.sensors.filesystem import FileSensor

wait_for_file = FileSensor(
    task_id='wait_for_data_file',
    filepath='/data/crypto/latest.csv',
    poke_interval=60,
    timeout=3600,
    dag=dag
)
```

### 4. Branching for Conditional Logic

```python
from airflow.operators.python import BranchPythonOperator

def decide_processing(**context):
    data_size = context['task_instance'].xcom_pull(key='data_size')
    if data_size > 1000000:
        return 'heavy_processing'
    else:
        return 'light_processing'

branch = BranchPythonOperator(
    task_id='branch_task',
    python_callable=decide_processing,
    dag=dag
)
```

### 5. Callbacks for Notifications

```python
def on_failure_callback(context):
    """Send Slack/Email notification on failure"""
    send_alert(f"DAG {context['dag'].dag_id} failed!")

dag = DAG(
    'crypto_etl_dag',
    default_args={
        'on_failure_callback': on_failure_callback,
    }
)
```

---

## ÔøΩüîß T√πy Ch·ªânh

### Crawler Configuration

```python
# config/crawler_config.yaml
sources:
  coingecko:
    enabled: true
    symbols: 100
    rate_limit: 50  # requests per minute

  binance:
    enabled: true
    symbols: ['BTCUSDT', 'ETHUSDT', 'BNBUSDT']
    intervals: ['1m', '5m', '1h', '1d']

storage:
  output_format: 'parquet'  # csv, json, parquet
  compression: 'snappy'

kafka:
  enabled: true
  bootstrap_servers: 'localhost:9092'
  topics:
    ohlcv: 'crypto.ohlcv.1m'
    trades: 'crypto.trades.raw'
```

### Thay ƒë·ªïi s·ªë l∆∞·ª£ng coins crawl:

```python
# Trong test.py ho·∫∑c crypto_crawler_advanced.py
top_coins = crawler.get_top_cryptocurrencies(limit=200)  # Thay ƒë·ªïi t·ª´ 50 -> 200
```

### Thay ƒë·ªïi timeframe d·ªØ li·ªáu l·ªãch s·ª≠:

```python
# Crawl 90 ng√†y thay v√¨ 30 ng√†y
historical = crawler.get_historical_data('bitcoin', days=90)
```

### Airflow DAG Schedule Customization:

```python
# Ch·∫°y m·ªói 15 ph√∫t
schedule_interval='*/15 * * * *'

# Ch·∫°y m·ªói gi·ªù
schedule_interval='0 * * * *'

# Ch·∫°y m·ªói ng√†y l√∫c 2:30 AM
schedule_interval='30 2 * * *'

# Ch·∫°y m·ªói th·ª© 2 l√∫c 8:00 AM
schedule_interval='0 8 * * 1'
```

### Thay ƒë·ªïi trading pair tr√™n Binance:

```python
# Crawl Ethereum thay v√¨ Bitcoin
df = crawler.crawl_binance_klines('ETHUSDT', interval='1h', limit=100)
```

### Thay ƒë·ªïi interval cho scheduled crawler:

```python
# Ch·∫°y m·ªói 30 ph√∫t thay v√¨ 60 ph√∫t
crawler.run_scheduler(interval_minutes=30)
```

## üìà Ph√¢n T√≠ch D·ªØ Li·ªáu

### Load v√† ph√¢n t√≠ch d·ªØ li·ªáu ƒë√£ crawl:

```python
import pandas as pd

# Load data
df = pd.read_csv('crypto_data_advanced/coingecko_markets_YYYYMMDD_HHMMSS.csv')

# Top 10 gainers 24h
top_gainers = df.nlargest(10, 'price_change_percentage_24h')
print(top_gainers[['name', 'symbol', 'current_price', 'price_change_percentage_24h']])

# Top 10 losers 24h
top_losers = df.nsmallest(10, 'price_change_percentage_24h')
print(top_losers[['name', 'symbol', 'current_price', 'price_change_percentage_24h']])

# Coins with RSI > 70 (overbought)
btc_data = pd.read_csv('crypto_data_advanced/btc_klines_with_indicators_YYYYMMDD_HHMMSS.csv')
overbought = btc_data[btc_data['RSI'] > 70]
print(f"Overbought periods: {len(overbought)}")
```

## üîÑ Scheduled Crawling

### Ch·∫°y crawler t·ª± ƒë·ªông m·ªói gi·ªù:

```python
from crypto_crawler_advanced import AdvancedCryptoCrawler

crawler = AdvancedCryptoCrawler()
crawler.run_scheduler(interval_minutes=60)  # Crawl m·ªói 60 ph√∫t
```

### Ho·∫∑c s·ª≠ d·ª•ng cron job (Linux/macOS):

```bash
# Ch·ªânh s·ª≠a crontab
crontab -e

# Th√™m d√≤ng n√†y ƒë·ªÉ ch·∫°y m·ªói gi·ªù
0 * * * * cd /path/to/project/data && /usr/bin/python3 crypto_crawler_advanced.py
```

## üìù Logging

Logs ƒë∆∞·ª£c l∆∞u trong file `crypto_crawler.log`:

```bash
# Xem logs real-time
tail -f crypto_crawler.log

# Xem 100 d√≤ng cu·ªëi
tail -n 100 crypto_crawler.log
```

## ‚ö†Ô∏è L∆∞u √ù Quan Tr·ªçng

### Rate Limits

**CoinGecko Free API:**

- ~10-50 requests/ph√∫t
- S·ª≠ d·ª•ng `time.sleep()` gi·ªØa c√°c requests

**Binance API:**

- 1200 requests/ph√∫t cho IP
- 6000 requests/ph√∫t cho UID

### Best Practices

1. **Lu√¥n check response status:**

   ```python
   response.raise_for_status()
   ```

2. **S·ª≠ d·ª•ng timeout:**

   ```python
   requests.get(url, timeout=30)
   ```

3. **Handle exceptions:**

   ```python
   try:
       data = crawler.crawl_data()
   except Exception as e:
       logger.error(f"Error: {e}")
   ```

4. **Rate limiting:**
   ```python
   time.sleep(2)  # Delay 2 gi√¢y gi·ªØa c√°c requests
   ```

## üéì Examples

### Example 1: Crawl v√† analyze Bitcoin

```python
from crypto_crawler_advanced import AdvancedCryptoCrawler

crawler = AdvancedCryptoCrawler()

# Crawl Bitcoin data
df = crawler.crawl_binance_klines('BTCUSDT', interval='1d', limit=365)

# Calculate indicators
df_with_indicators = crawler.calculate_technical_indicators(df)

# Save
crawler.save_data(df_with_indicators, 'btc_yearly_analysis')

# Analyze
print(f"Current RSI: {df_with_indicators['RSI'].iloc[-1]:.2f}")
print(f"MACD: {df_with_indicators['MACD'].iloc[-1]:.2f}")
```

### Example 2: Monitor market sentiment

```python
crawler = AdvancedCryptoCrawler()

# Get market data
df = crawler.crawl_coingecko_markets(limit=100)

# Analyze sentiment
sentiment = crawler.analyze_market_sentiment(df)

print(f"Gainers: {sentiment['gainers']}")
print(f"Losers: {sentiment['losers']}")
print(f"Average 24h change: {sentiment['avg_change_24h']:.2f}%")
```

### Example 3: Compare multiple coins

```python
coins = ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'SOLUSDT']

for coin in coins:
    df = crawler.crawl_binance_klines(coin, interval='1h', limit=24)
    if df is not None:
        price_change = ((df['close'].iloc[-1] - df['close'].iloc[0]) / df['close'].iloc[0]) * 100
        print(f"{coin}: {price_change:+.2f}% (24h)")
    time.sleep(1)
```

## üêõ Troubleshooting

### L·ªói: Module not found

```bash
pip install requests pandas numpy schedule
```

### L·ªói: Rate limit exceeded

```python
# TƒÉng delay gi·ªØa c√°c requests
time.sleep(5)  # Thay v√¨ 2 gi√¢y
```

### L·ªói: Connection timeout

```python
# TƒÉng timeout
response = requests.get(url, timeout=60)  # Thay v√¨ 30
```

## üìö T√†i Li·ªáu API

- [CoinGecko API Docs](https://www.coingecko.com/en/api/documentation)
- [Binance API Docs](https://binance-docs.github.io/apidocs/spot/en/)

## üí° Tips & Tricks

1. **L∆∞u d·ªØ li·ªáu v·ªõi timestamp** ƒë·ªÉ kh√¥ng b·ªã ghi ƒë√®
2. **S·ª≠ d·ª•ng parquet format** cho files l·ªõn (nhanh h∆°n CSV)
3. **Crawl theo batch** ƒë·ªÉ tr√°nh rate limit
4. **Backup d·ªØ li·ªáu ƒë·ªãnh k·ª≥**
5. **Monitor disk space** khi crawl li√™n t·ª•c

## üöÄ Next Steps

- [ ] T√≠ch h·ª£p th√™m exchanges (Coinbase, Kraken, FTX)
- [ ] Th√™m real-time WebSocket streaming
- [ ] X√¢y d·ª±ng database (PostgreSQL/MongoDB)
- [ ] T·∫°o dashboard visualization (Streamlit/Dash)
- [ ] Machine Learning models cho price prediction
- [ ] Alert system cho price movements

---

**Happy Crawling! üéâ**
