# FROM apache/spark:3.5.0-python3
# USER root
# WORKDIR /opt/spark
# COPY requirements.txt .
# RUN pip install --no-cache-dir -r requirements.txt
# COPY jars/gcs-connector-hadoop3-latest.jar /opt/spark/jars/
# COPY jars/spark-bigquery-with-dependencies_2.12-0.36.1.jar /opt/spark/jars/
# COPY code/write_to_big_query.py /opt/spark/work-dir/
# COPY start_date.txt /opt/spark/
# CMD ["/opt/spark/bin/spark-submit", \
#      "--master", "k8s://https://kubernetes.default.svc:443", \
#      "--deploy-mode", "client", \
#      "--name", "Spark-Batch-ETL", \
#      "--conf", "spark.kubernetes.container.image=docker.io/helloimgnud/spark-batch:latest", \
#      "--conf", "spark.kubernetes.namespace=crypto-pipeline", \
#      "--conf", "spark.executor.instances=3", \
#      "--conf", "spark.executor.memory=2g", \
#      "--conf", "spark.executor.cores=2", \
#      "--jars", "/opt/spark/jars/gcs-connector-hadoop3-latest.jar,/opt/spark/jars/spark-bigquery-with-dependencies_2.12-0.36.1.jar", \
#      "/opt/spark/work-dir/write_to_big_query.py"]


FROM apache/spark:3.5.0-python3
USER root

# Cài đặt thư viện Python cần thiết
WORKDIR /opt/spark
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Tạo thư mục chứa code và jars
RUN mkdir -p /opt/spark/jars /opt/spark/code

# Copy Jars và Code vào đúng vị trí
COPY jars/gcs-connector-hadoop3-latest.jar /opt/spark/jars/
COPY jars/spark-bigquery-with-dependencies_2.12-0.36.1.jar /opt/spark/jars/
COPY code/write_to_big_query.py /opt/spark/code/
COPY start_date.txt /opt/spark/

# Quan trọng: Cấp quyền ghi cho file start_date
RUN chmod 777 /opt/spark/start_date.txt

# Không cần CMD phức tạp vì Airflow sẽ truyền lệnh vào
CMD ["/bin/bash"]
