# Enterprise-Grade Big Data Project: Cryptocurrency Market Intelligence Platform

## Production-Ready Lambda Architecture System

---

## I. BUSINESS PROBLEM & VALUE PROPOSITION

### Real-World Problem Statement

**"Cryptocurrency Market Intelligence and Risk Management Platform for Institutional Investors"**

#### Business Context

T·ªï ch·ª©c ƒë·∫ßu t∆∞ v·ªõi portfolio $100M+ trong crypto c·∫ßn:

1. **Real-time Market Monitoring**: Theo d√µi 1000+ coins, 50+ exchanges, 24/7
2. **Risk Management**: Ph√°t hi·ªán anomaly, flash crashes, market manipulation
3. **Alpha Generation**: T√¨m trading opportunities qua data analysis
4. **Compliance**: B√°o c√°o theo quy ƒë·ªãnh, audit trail ƒë·∫ßy ƒë·ªß
5. **Multi-Asset Strategy**: Correlation analysis, portfolio optimization

#### Pain Points Hi·ªán T·∫°i

‚ùå Manual monitoring ‚Üí Missed opportunities  
‚ùå Delayed data ‚Üí Poor execution prices  
‚ùå No anomaly detection ‚Üí Unexpected losses  
‚ùå Siloed data ‚Üí Incomplete market view  
‚ùå No backtesting ‚Üí Unvalidated strategies

#### Solution Value

‚úÖ **$2M+/year** saved t·ª´ better execution prices  
‚úÖ **60% reduction** in risk exposure t·ª´ early warnings  
‚úÖ **3x faster** decision making v·ªõi real-time insights  
‚úÖ **40% increase** in trading efficiency  
‚úÖ **Full compliance** v·ªõi regulatory requirements

### Target Users

1. **Portfolio Managers**: Real-time portfolio analytics, risk metrics
2. **Traders**: Price alerts, market signals, order flow analysis
3. **Risk Officers**: Exposure monitoring, VaR calculations, stress testing
4. **Compliance Team**: Transaction monitoring, reporting, audit logs
5. **Data Scientists**: Feature store, model training infrastructure

---

---

## II. ENTERPRISE LAMBDA ARCHITECTURE

### Architecture Philosophy

**"Immutable Data, Recomputable Views, Human Fault-Tolerant"**

#### Why Lambda over Kappa?

| Aspect                  | Lambda             | Kappa               | Our Choice              |
| ----------------------- | ------------------ | ------------------- | ----------------------- |
| **Batch Reprocessing**  | Easy, full history | Hard, replay needed | ‚úÖ Lambda               |
| **Data Correction**     | Batch recompute    | Complex replay      | ‚úÖ Lambda               |
| **Historical Analysis** | Optimized batch    | Stream replay       | ‚úÖ Lambda               |
| **Code Complexity**     | Higher             | Lower               | Acceptable tradeoff     |
| **Audit & Compliance**  | Full history       | Limited             | ‚úÖ Critical for finance |

### Detailed Architecture Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         DATA INGESTION LAYER                                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ  Binance    ‚îÇ  ‚îÇ  Coinbase   ‚îÇ  ‚îÇ  CoinGecko  ‚îÇ  ‚îÇ   Twitter    ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  WebSocket  ‚îÇ  ‚îÇ  WebSocket  ‚îÇ  ‚îÇ     API     ‚îÇ  ‚îÇ  Streaming   ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ   (OHLCV)   ‚îÇ  ‚îÇ  (Trades)   ‚îÇ  ‚îÇ  (Market)   ‚îÇ  ‚îÇ  (Sentiment) ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ         ‚îÇ                 ‚îÇ                 ‚îÇ                 ‚îÇ                  ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ                                    ‚îÇ                                             ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                           ‚îÇ
‚îÇ                    ‚îÇ   Apache Kafka Cluster          ‚îÇ                           ‚îÇ
‚îÇ                    ‚îÇ   - 3 brokers, RF=3             ‚îÇ                           ‚îÇ
‚îÇ                    ‚îÇ   - 12 topics, partitioned      ‚îÇ                           ‚îÇ
‚îÇ                    ‚îÇ   - Retention: 7 days           ‚îÇ                           ‚îÇ
‚îÇ                    ‚îÇ   - Throughput: 100K msg/sec   ‚îÇ                           ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                     ‚îÇ
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ                                           ‚îÇ
                ‚ñº                                           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         BATCH LAYER (Œª)               ‚îÇ   ‚îÇ         SPEED LAYER (Œ¥)              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                       ‚îÇ   ‚îÇ                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Kafka ‚Üí HDFS Ingestion         ‚îÇ ‚îÇ   ‚îÇ  ‚îÇ  Spark Structured Streaming    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - NiFi/Gobblin ETL             ‚îÇ ‚îÇ   ‚îÇ  ‚îÇ  - 3 streaming jobs            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Avro/Parquet format          ‚îÇ ‚îÇ   ‚îÇ  ‚îÇ  - Watermarking: 15 min        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Partitioned: /year/month/day ‚îÇ ‚îÇ   ‚îÇ  ‚îÇ  - Checkpointing enabled       ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                ‚îÇ                      ‚îÇ   ‚îÇ                ‚îÇ                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  HDFS Data Lake                 ‚îÇ ‚îÇ   ‚îÇ  ‚îÇ  Stream Processing             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Capacity: 100TB+             ‚îÇ ‚îÇ   ‚îÇ  ‚îÇ  - Window: 1min, 5min, 1h      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Replication: 3               ‚îÇ ‚îÇ   ‚îÇ  ‚îÇ  - Aggregations: OHLCV         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Retention: 5 years           ‚îÇ ‚îÇ   ‚îÇ  ‚îÇ  - Joins: Enrichment           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                  ‚îÇ ‚îÇ   ‚îÇ  ‚îÇ  - Output: Redis + Kafka       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  /crypto/raw/                   ‚îÇ ‚îÇ   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ  ‚îÇ    - trades/                    ‚îÇ ‚îÇ   ‚îÇ                ‚îÇ                     ‚îÇ
‚îÇ  ‚îÇ    - orderbook/                 ‚îÇ ‚îÇ   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ    - market_data/               ‚îÇ ‚îÇ   ‚îÇ  ‚îÇ  Real-time Views (Redis)       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  /crypto/processed/             ‚îÇ ‚îÇ   ‚îÇ  ‚îÇ  - TTL: 1 hour                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    - ohlcv_1m/                  ‚îÇ ‚îÇ   ‚îÇ  ‚îÇ  - Memory: 64GB                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    - features/                  ‚îÇ ‚îÇ   ‚îÇ  ‚îÇ  - Cluster: 3 nodes            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    - aggregates/                ‚îÇ ‚îÇ   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ   ‚îÇ                                      ‚îÇ
‚îÇ                ‚îÇ                      ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ  Apache Spark Batch Jobs        ‚îÇ ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ  - Cluster: 1 master + 10 worker‚îÇ ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ  - Memory: 512GB total          ‚îÇ ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ  - Cores: 160 vCPUs             ‚îÇ ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ                                  ‚îÇ ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ  Jobs:                          ‚îÇ ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ  1. Daily OHLCV Aggregation    ‚îÇ ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ  2. Feature Engineering         ‚îÇ ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ  3. Correlation Computation     ‚îÇ ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ  4. Model Training              ‚îÇ ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ  5. Historical Analytics        ‚îÇ ‚îÇ                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                  ‚îÇ
‚îÇ                ‚îÇ                      ‚îÇ                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ  Batch Views (HBase)            ‚îÇ ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ  - Tables: 10+                  ‚îÇ ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ  - Row Key: symbol_timestamp    ‚îÇ ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ  - Region Servers: 5            ‚îÇ ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ  - Compression: Snappy          ‚îÇ ‚îÇ                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ                  ‚îÇ
‚îÇ                ‚îÇ                      ‚îÇ                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
                 ‚îÇ                                          ‚îÇ
                 ‚îÇ      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ      ‚îÇ
                 ‚ñº      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     SERVING LAYER (œÉ)                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Query Service (FastAPI)                                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Pods: 10 replicas                                           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Load Balancer: Nginx Ingress                                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Rate Limit: 1000 req/sec                                    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Endpoints:                                                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - GET /api/v1/ohlcv/{symbol}?from={ts}&to={ts}              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - GET /api/v1/realtime/{symbol}                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - GET /api/v1/analytics/correlation                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - POST /api/v1/predictions                                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - GET /api/v1/alerts                                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - WebSocket /ws/stream/{symbol}                              ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                       ‚îÇ                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Query Optimization Layer                                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Cache: Redis (hot data, 5min TTL)                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Batch queries: HBase scan                                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Real-time queries: Redis get                                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Merge logic: Union batch + streaming views                  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  ANALYTICS & ML LAYER                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   Feature Store  ‚îÇ  ‚îÇ  Model Registry  ‚îÇ  ‚îÇ  Experiment      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   (Feast/Delta)  ‚îÇ  ‚îÇ    (MLflow)      ‚îÇ  ‚îÇ   Tracking       ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ           ‚îÇ                     ‚îÇ                      ‚îÇ            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  ML Pipeline (Spark MLlib + Custom Models)                    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Models:                                                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  1. Price Prediction (GBT Regressor)        - RMSE: 2.3%     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  2. Trend Classification (Random Forest)    - Acc: 78%       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  3. Anomaly Detection (Isolation Forest)    - F1: 0.85       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  4. Portfolio Optimization (Convex Opt)     - Sharpe: 2.1x   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  5. Sentiment Analysis (BERT fine-tuned)    - AUC: 0.89      ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  VISUALIZATION & MONITORING                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Business Dashboard (React + Plotly)                          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Real-time price charts (WebSocket)                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Portfolio analytics                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Risk metrics (VaR, CVaR)                                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Alert management                                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - ML model insights                                          ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Ops Dashboard (Grafana)                                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Kafka lag monitoring                                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Spark job metrics                                          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - HDFS capacity                                              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - API latency (p50, p95, p99)                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Error rates                                                ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  INFRASTRUCTURE LAYER                                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                      ‚îÇ
‚îÇ  Kubernetes Cluster (GKE/EKS/AKS)                                   ‚îÇ
‚îÇ  - 30 nodes (n1-standard-16)                                        ‚îÇ
‚îÇ  - Namespaces: ingestion, batch, streaming, serving, monitoring     ‚îÇ
‚îÇ  - Auto-scaling: HPA + Cluster Autoscaler                           ‚îÇ
‚îÇ  - Service Mesh: Istio                                              ‚îÇ
‚îÇ  - Secret Management: Vault                                         ‚îÇ
‚îÇ  - CI/CD: GitLab CI + ArgoCD                                        ‚îÇ
‚îÇ  - Monitoring: Prometheus + Grafana + Jaeger                        ‚îÇ
‚îÇ  - Logging: ELK Stack (Elasticsearch + Logstash + Kibana)          ‚îÇ
‚îÇ  - Backup: Velero                                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è III. STACK C√îNG NGH·ªÜ (ƒê√ÅP ·ª®NG Y√äU C·∫¶U)

### ‚úÖ Core Technologies (B·∫Øt Bu·ªôc)

#### 1. Apache Spark (PySpark)

**Vai tr√≤:** Data Processing Engine ch√≠nh

**Batch Processing:**

```python
# Spark SQL, DataFrame API
# Complex aggregations, window functions
# MLlib for predictions
# GraphFrames for correlation analysis
```

**Stream Processing:**

```python
# Structured Streaming
# Watermarking & late data handling
# Stateful operations
# Exactly-once semantics
```

#### 2. Distributed Storage: **HDFS**

**Vai tr√≤:** L∆∞u tr·ªØ d·ªØ li·ªáu l·ªãch s·ª≠

**Structure:**

```
/crypto_data/
‚îú‚îÄ‚îÄ /raw/
‚îÇ   ‚îú‚îÄ‚îÄ /daily/YYYY/MM/DD/
‚îÇ   ‚îî‚îÄ‚îÄ /hourly/YYYY/MM/DD/HH/
‚îú‚îÄ‚îÄ /processed/
‚îÇ   ‚îú‚îÄ‚îÄ /aggregated/
‚îÇ   ‚îî‚îÄ‚îÄ /features/
‚îî‚îÄ‚îÄ /ml_models/
    ‚îî‚îÄ‚îÄ /checkpoints/
```

#### 3. Message Queue: **Apache Kafka**

**Vai tr√≤:** Real-time data streaming

**Topics:**

```
- crypto.prices.raw          # Raw price updates
- crypto.prices.processed    # Cleaned data
- crypto.alerts              # Anomaly alerts
- crypto.predictions         # ML predictions
```

#### 4. NoSQL Database: **Apache HBase + Redis**

**HBase** (tr√™n HDFS):

- Time-series data storage
- Historical queries
- Batch view results

**Redis:**

- Real-time view caching
- Speed layer results
- Session storage

#### 5. Deployment: **Kubernetes**

**Components:**

```yaml
- Spark Master/Workers Pods
- Kafka Cluster
- HBase Region Servers
- Redis Cluster
- API Service Pods
- Dashboard Pods
```

---

## üìö IV. ƒê√ÅP ·ª®NG Y√äU C·∫¶U K·ª∏ THU·∫¨T SPARK

### 1. Complex Aggregations ‚úÖ

#### Window Functions

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import *

# T√≠nh moving averages v·ªõi window functions
window_7d = Window.partitionBy("symbol").orderBy("timestamp").rowsBetween(-6, 0)
window_30d = Window.partitionBy("symbol").orderBy("timestamp").rowsBetween(-29, 0)

df_with_ma = df.withColumn("ma_7", avg("close_price").over(window_7d)) \
               .withColumn("ma_30", avg("close_price").over(window_30d)) \
               .withColumn("price_change_7d",
                          (col("close_price") - first("close_price").over(window_7d)) /
                          first("close_price").over(window_7d) * 100)

# Ranking coins by volume
window_rank = Window.partitionBy("timestamp").orderBy(desc("volume_24h"))
df_ranked = df.withColumn("volume_rank", row_number().over(window_rank))
```

#### Pivot Operations

```python
# Pivot: Chuy·ªÉn d·ªØ li·ªáu t·ª´ long format sang wide format
# T·∫°o ma tr·∫≠n gi√° c·ªßa c√°c coins theo th·ªùi gian
pivot_df = df.groupBy("timestamp") \
    .pivot("symbol") \
    .agg(first("close_price").alias("price"))

# Unpivot: Chuy·ªÉn ng∆∞·ª£c l·∫°i
unpivot_df = pivot_df.selectExpr(
    "timestamp",
    "stack(50, 'BTC', BTC, 'ETH', ETH, ...) as (symbol, price)"
)
```

#### Custom Aggregation Functions (UDAF)

```python
from pyspark.sql.types import DoubleType

# Custom UDAF: T√≠nh Sharpe Ratio
@pandas_udf(DoubleType())
def calculate_sharpe_ratio(returns: pd.Series) -> float:
    if len(returns) < 2:
        return 0.0
    return (returns.mean() / returns.std()) * np.sqrt(252)

df_sharpe = df.groupBy("symbol") \
    .agg(calculate_sharpe_ratio(col("daily_return")).alias("sharpe_ratio"))
```

### 2. Advanced Transformations ‚úÖ

#### Multi-stage Transformations

```python
# Stage 1: Data Cleaning
df_cleaned = df.filter(col("price") > 0) \
    .filter(col("volume") > 0) \
    .dropDuplicates(["symbol", "timestamp"]) \
    .na.fill({"market_cap": 0})

# Stage 2: Feature Engineering
df_features = df_cleaned \
    .withColumn("log_price", log(col("price"))) \
    .withColumn("price_volatility", stddev("price").over(window_24h)) \
    .withColumn("volume_change",
                (col("volume") - lag("volume", 1).over(window_time)) /
                lag("volume", 1).over(window_time))

# Stage 3: Technical Indicators
df_indicators = df_features \
    .withColumn("rsi", calculate_rsi(col("close_price"))) \
    .withColumn("macd", calculate_macd(col("close_price"))) \
    .withColumn("bollinger_upper", col("ma_20") + 2 * col("stddev_20")) \
    .withColumn("bollinger_lower", col("ma_20") - 2 * col("stddev_20"))

# Stage 4: Signal Generation
df_signals = df_indicators \
    .withColumn("buy_signal",
                when((col("rsi") < 30) & (col("macd") > col("signal_line")), 1)
                .otherwise(0)) \
    .withColumn("sell_signal",
                when((col("rsi") > 70) & (col("macd") < col("signal_line")), 1)
                .otherwise(0))
```

#### Custom UDFs

```python
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType, DoubleType

# UDF: Ph√¢n lo·∫°i trend
@udf(returnType=StringType())
def classify_trend(ma_7, ma_30, rsi):
    if ma_7 > ma_30 and rsi > 50:
        return "STRONG_BULLISH"
    elif ma_7 > ma_30 and rsi <= 50:
        return "WEAK_BULLISH"
    elif ma_7 < ma_30 and rsi < 50:
        return "STRONG_BEARISH"
    else:
        return "WEAK_BEARISH"

df_trends = df.withColumn("trend", classify_trend(col("ma_7"), col("ma_30"), col("rsi")))

# UDF: T√≠nh correlation score
@udf(returnType=DoubleType())
def calculate_correlation_score(price_changes: list) -> float:
    if len(price_changes) < 2:
        return 0.0
    return np.corrcoef(price_changes, range(len(price_changes)))[0, 1]
```

### 3. Join Operations ‚úÖ

#### Broadcast Join (Small-to-Large)

```python
# Broadcast small dimension table (coin metadata)
coin_metadata = spark.read.parquet("hdfs:///crypto/metadata/coins")
coin_metadata_broadcast = broadcast(coin_metadata)

# Join v·ªõi large fact table (price data)
df_enriched = df_prices.join(
    coin_metadata_broadcast,
    df_prices.symbol == coin_metadata_broadcast.coin_symbol,
    "left"
)
```

#### Sort-Merge Join (Large-to-Large)

```python
# Join 2 large datasets: prices v√† volumes
df_prices_partitioned = df_prices.repartition(200, "symbol", "date")
df_volumes_partitioned = df_volumes.repartition(200, "symbol", "date")

df_joined = df_prices_partitioned.join(
    df_volumes_partitioned,
    ["symbol", "date"],
    "inner"
).sortWithinPartitions("symbol", "timestamp")
```

#### Multiple Joins Optimization

```python
# Join multiple data sources
df_result = df_prices \
    .join(df_volumes, ["symbol", "timestamp"], "inner") \
    .join(broadcast(df_market_cap), ["symbol", "date"], "left") \
    .join(df_social_sentiment, ["symbol", "date"], "left") \
    .join(broadcast(df_exchanges),
          df_prices.exchange_id == df_exchanges.id, "left")

# Optimize v·ªõi join hints
df_optimized = df_prices.hint("merge") \
    .join(df_volumes.hint("merge"), ["symbol", "timestamp"])
```

### 4. Performance Optimization ‚úÖ

#### Partition Pruning & Bucketing

```python
# Partitioning by date and symbol
df.write \
    .partitionBy("year", "month", "day", "symbol") \
    .mode("append") \
    .parquet("hdfs:///crypto/partitioned/prices")

# Bucketing for frequent joins
df.write \
    .bucketBy(100, "symbol") \
    .sortBy("timestamp") \
    .mode("overwrite") \
    .saveAsTable("crypto_prices_bucketed")

# Query with partition pruning
df_filtered = spark.read.parquet("hdfs:///crypto/partitioned/prices") \
    .filter(col("year") == 2025) \
    .filter(col("month") == 10) \
    .filter(col("symbol").isin(["BTC", "ETH"]))
```

#### Caching Strategy

```python
# Cache hot data
df_popular_coins = df.filter(col("market_cap_rank") <= 20).cache()
df_popular_coins.count()  # Trigger caching

# Persist v·ªõi storage level kh√°c nhau
df_large.persist(StorageLevel.MEMORY_AND_DISK_SER)

# Unpersist khi kh√¥ng c·∫ßn
df_popular_coins.unpersist()
```

#### Query Optimization

```python
# Enable Adaptive Query Execution
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")

# Enable Cost-Based Optimization
spark.conf.set("spark.sql.cbo.enabled", "true")
spark.conf.set("spark.sql.statistics.histogram.enabled", "true")

# Analyze execution plan
df.explain(mode="extended")
df.explain(mode="cost")
```

### 5. Streaming Processing ‚úÖ

#### Structured Streaming v·ªõi Kafka

```python
# Read from Kafka
df_stream = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "crypto.prices.raw") \
    .option("startingOffsets", "latest") \
    .load()

# Parse JSON data
df_parsed = df_stream.selectExpr("CAST(value AS STRING)") \
    .select(from_json(col("value"), price_schema).alias("data")) \
    .select("data.*")

# Apply transformations
df_processed = df_parsed \
    .withWatermark("timestamp", "10 minutes") \
    .groupBy(
        window("timestamp", "5 minutes", "1 minute"),
        "symbol"
    ) \
    .agg(
        avg("price").alias("avg_price"),
        max("price").alias("high"),
        min("price").alias("low"),
        sum("volume").alias("total_volume"),
        count("*").alias("tick_count")
    )
```

#### Watermarking & Late Data

```python
# Watermarking for handling late data
df_with_watermark = df_stream \
    .withWatermark("event_time", "30 minutes")

# Aggregate v·ªõi late data handling
df_aggregated = df_with_watermark \
    .groupBy(
        window("event_time", "1 hour", "15 minutes"),
        "symbol"
    ) \
    .agg(
        avg("price").alias("avg_price"),
        stddev("price").alias("price_volatility")
    ) \
    .filter(col("price_volatility") > 0.05)  # Filter high volatility
```

#### State Management

```python
from pyspark.sql.streaming import GroupState, GroupStateTimeout

# Custom stateful processing
def update_price_state(symbol, values, state: GroupState):
    """Maintain running statistics per symbol"""
    if state.hasTimedOut:
        # Handle timeout
        state.remove()
        return None

    # Get current state
    if state.exists:
        current_state = state.get
    else:
        current_state = {"count": 0, "sum": 0.0, "max": 0.0}

    # Update state
    for value in values:
        current_state["count"] += 1
        current_state["sum"] += value.price
        current_state["max"] = max(current_state["max"], value.price)

    # Set new state with timeout
    state.update(current_state)
    state.setTimeoutDuration("1 hour")

    return (symbol, current_state["sum"] / current_state["count"], current_state["max"])

df_stateful = df_stream.groupByKey(lambda x: x.symbol) \
    .mapGroupsWithState(update_price_state, GroupStateTimeout.ProcessingTimeTimeout)
```

#### Output Modes & Exactly-Once

```python
# Complete mode: Full result table
query_complete = df_processed.writeStream \
    .outputMode("complete") \
    .format("memory") \
    .queryName("aggregates_table") \
    .start()

# Append mode: Only new rows
query_append = df_processed.writeStream \
    .outputMode("append") \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("topic", "crypto.prices.processed") \
    .option("checkpointLocation", "hdfs:///checkpoints/prices") \
    .start()

# Update mode: Updated rows only
query_update = df_processed.writeStream \
    .outputMode("update") \
    .format("parquet") \
    .option("path", "hdfs:///crypto/streaming/output") \
    .option("checkpointLocation", "hdfs:///checkpoints/streaming") \
    .trigger(processingTime="30 seconds") \
    .start()

# Exactly-once semantics v·ªõi idempotent writes
query_exactly_once = df_processed.writeStream \
    .foreachBatch(lambda batch_df, batch_id:
        batch_df.write \
            .format("jdbc") \
            .option("url", jdbc_url) \
            .option("dbtable", f"crypto_stream_{batch_id}") \
            .mode("overwrite") \
            .save()
    ) \
    .start()
```

### 6. Advanced Analytics ‚úÖ

#### Machine Learning v·ªõi MLlib

```python
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.regression import GBTRegressor, RandomForestRegressor
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

# Feature Engineering
feature_cols = ["ma_7", "ma_30", "rsi", "macd", "volume_change",
                "price_volatility", "market_cap", "volume_24h"]
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
scaler = StandardScaler(inputCol="features", outputCol="scaled_features")

# Model Pipeline
gbt = GBTRegressor(
    featuresCol="scaled_features",
    labelCol="price_next_hour",
    maxIter=100,
    maxDepth=5
)

pipeline = Pipeline(stages=[assembler, scaler, gbt])

# Cross-Validation
param_grid = ParamGridBuilder() \
    .addGrid(gbt.maxDepth, [3, 5, 7]) \
    .addGrid(gbt.maxIter, [50, 100, 150]) \
    .build()

evaluator = RegressionEvaluator(
    labelCol="price_next_hour",
    predictionCol="prediction",
    metricName="rmse"
)

cv = CrossValidator(
    estimator=pipeline,
    estimatorParamMaps=param_grid,
    evaluator=evaluator,
    numFolds=5
)

# Train model
cv_model = cv.fit(train_df)
predictions = cv_model.transform(test_df)

# Evaluate
rmse = evaluator.evaluate(predictions)
print(f"RMSE: {rmse}")
```

#### Graph Processing v·ªõi GraphFrames

```python
from graphframes import GraphFrame

# Create vertices (coins)
vertices = df_coins.select("symbol", "name", "market_cap") \
    .withColumnRenamed("symbol", "id")

# Create edges (correlations between coins)
edges = df_correlations.select(
    col("symbol_1").alias("src"),
    col("symbol_2").alias("dst"),
    col("correlation").alias("relationship")
).filter(col("correlation") > 0.7)

# Create graph
graph = GraphFrame(vertices, edges)

# PageRank: Find most influential coins
page_rank = graph.pageRank(resetProbability=0.15, maxIter=10)
page_rank.vertices.orderBy(desc("pagerank")).show()

# Find connected components (groups of correlated coins)
connected = graph.connectedComponents()
connected.groupBy("component") \
    .agg(collect_list("id").alias("coins")) \
    .show(truncate=False)

# Shortest paths
paths = graph.shortestPaths(landmarks=["BTC", "ETH"])
```

#### Statistical Computations

```python
from pyspark.ml.stat import Correlation, ChiSquareTest
from pyspark.mllib.stat import Statistics

# Correlation matrix
vector_col = "features"
corr_matrix = Correlation.corr(df, vector_col, "pearson").head()
print("Pearson correlation matrix:\n", corr_matrix[0])

# Chi-square test
chi_result = ChiSquareTest.test(df, "features", "label").head()
print(f"Chi-square statistic: {chi_result.statistic}")
print(f"p-values: {chi_result.pValues}")

# Hypothesis testing
rdd = df.select("price_change").rdd.map(lambda x: x[0])
stats = Statistics.colStats(rdd)
print(f"Mean: {stats.mean()}")
print(f"Variance: {stats.variance()}")
```

#### Time Series Analysis

```python
from pyspark.sql.functions import lag, lead

# Create lagged features for time series
df_ts = df.withColumn("price_lag_1", lag("price", 1).over(window_time)) \
    .withColumn("price_lag_2", lag("price", 2).over(window_time)) \
    .withColumn("price_lag_7", lag("price", 7).over(window_time)) \
    .withColumn("price_lead_1", lead("price", 1).over(window_time))

# Calculate autocorrelation
def calculate_acf(df, column, lags=10):
    acf_results = []
    for lag_val in range(1, lags + 1):
        df_lagged = df.withColumn(f"lag_{lag_val}",
                                   lag(column, lag_val).over(window_time))
        correlation = df_lagged.stat.corr(column, f"lag_{lag_val}")
        acf_results.append((lag_val, correlation))
    return acf_results

# Seasonal decomposition
df_seasonal = df.withColumn("day_of_week", dayofweek("timestamp")) \
    .withColumn("hour", hour("timestamp")) \
    .groupBy("symbol", "day_of_week", "hour") \
    .agg(avg("price").alias("avg_price_seasonal"))
```

---

## üìã V. IMPLEMENTATION PLAN

### Phase 1: Setup & Infrastructure (Week 1-2)

```
‚úÖ Setup Kubernetes cluster
‚úÖ Deploy HDFS
‚úÖ Deploy Kafka cluster
‚úÖ Deploy HBase + Redis
‚úÖ Setup Spark on K8s
‚úÖ CI/CD pipeline
```

### Phase 2: Data Collection & Batch Layer (Week 3-4)

```
‚úÖ Implement data collectors (CoinGecko, Binance APIs)
‚úÖ Kafka producers
‚úÖ HDFS data ingestion
‚úÖ Spark batch processing v·ªõi all transformations
‚úÖ Complex aggregations & window functions
‚úÖ HBase batch views
```

### Phase 3: Speed Layer & Streaming (Week 5-6)

```
‚úÖ Structured Streaming implementation
‚úÖ Real-time processing v·ªõi Kafka
‚úÖ Watermarking & state management
‚úÖ Redis real-time views
‚úÖ Exactly-once processing
```

### Phase 4: Analytics & ML (Week 7-8)

```
‚úÖ Technical indicators calculation
‚úÖ MLlib price prediction models
‚úÖ GraphFrames correlation analysis
‚úÖ Anomaly detection
‚úÖ Time series forecasting
```

### Phase 5: Serving Layer & API (Week 9-10)

```
‚úÖ Merge batch & speed views
‚úÖ REST API v·ªõi FastAPI
‚úÖ Query optimization
‚úÖ Caching strategy
```

### Phase 6: Visualization & Testing (Week 11-12)

```
‚úÖ Dashboard implementation
‚úÖ Real-time charts
‚úÖ Performance testing
‚úÖ Documentation
‚úÖ Demo preparation
```

---

## üìä VI. DEMO SCENARIOS

### Scenario 1: Historical Analysis (Batch Layer)

```
Input: 2 years of Bitcoin price data
Process:
  - Complex aggregations v·ªõi window functions
  - Pivot operations for correlation matrix
  - Custom UDAF for risk metrics
  - Join v·ªõi market sentiment data
Output: Comprehensive historical report
```

### Scenario 2: Real-time Monitoring (Speed Layer)

```
Input: Live price stream from Binance
Process:
  - Structured Streaming v·ªõi watermarking
  - Stateful processing for running statistics
  - Anomaly detection v·ªõi trained model
  - Alert generation
Output: Real-time dashboard + alerts
```

### Scenario 3: Price Prediction (Advanced Analytics)

```
Input: Historical + real-time features
Process:
  - Feature engineering v·ªõi multiple transformations
  - MLlib model training v·ªõi cross-validation
  - Hyperparameter tuning
  - Model evaluation
Output: Price predictions v·ªõi confidence intervals
```

### Scenario 4: Market Correlation (Graph Analytics)

```
Input: Price movements of top 100 coins
Process:
  - Calculate pairwise correlations
  - Build correlation graph
  - GraphFrames PageRank
  - Community detection
Output: Correlation network visualization
```

---

## üéØ VII. ƒêI·ªÇM M·∫†NH C·ª¶A SOLUTION

### ‚úÖ ƒê√°p ·ª®ng 100% Y√™u C·∫ßu K·ªπ Thu·∫≠t

| Y√™u c·∫ßu                  | Implementation                   | ƒêi·ªÉm N·ªïi B·∫≠t            |
| ------------------------ | -------------------------------- | ----------------------- |
| **Apache Spark**         | PySpark + Structured Streaming   | Full API coverage       |
| **Distributed Storage**  | HDFS 3.x                         | Partitioned + Bucketed  |
| **Message Queue**        | Kafka 3.x                        | 3 topics, replication=3 |
| **NoSQL**                | HBase + Redis                    | Dual storage strategy   |
| **Deployment**           | Kubernetes                       | Production-ready        |
| **Complex Aggregations** | Window, Pivot, UDAF              | ‚úÖ Advanced             |
| **Transformations**      | Multi-stage + UDF                | ‚úÖ Comprehensive        |
| **Join Operations**      | Broadcast + Sort-Merge           | ‚úÖ Optimized            |
| **Performance**          | Partition + Cache + CBO          | ‚úÖ Tuned                |
| **Streaming**            | Watermark + State + Exactly-once | ‚úÖ Complete             |
| **Advanced Analytics**   | MLlib + GraphFrames + Stats      | ‚úÖ Full suite           |

### ‚úÖ Gi√° Tr·ªã Th·ª±c T·∫ø

- √Åp d·ª•ng cho trading decisions
- Risk management
- Portfolio optimization
- Market research

### ‚úÖ Kh·∫£ NƒÉng M·ªü R·ªông

- Horizontal scaling v·ªõi K8s
- Add th√™m data sources d·ªÖ d√†ng
- Extensible architecture

---

## üìñ VIII. T√ÄI LI·ªÜU THAM KH·∫¢O

### Code Examples

- T·∫•t c·∫£ code examples ƒë√£ ƒë∆∞·ª£c provide ·ªü tr√™n
- C√≥ th·ªÉ ch·∫°y tr·ª±c ti·∫øp tr√™n Spark cluster

### Architecture Diagrams

- Lambda Architecture diagram
- Data flow diagrams
- K8s deployment architecture

### Performance Benchmarks

- Throughput metrics
- Latency measurements
- Resource utilization

---

## üöÄ IX. NEXT STEPS

1. **Review & Approve Architecture** ‚úÖ
2. **Setup Development Environment**
3. **Start Implementation Phase 1**
4. **Weekly Progress Tracking**
5. **Demo Preparation**

---

**K·∫øt lu·∫≠n:** ƒê√¢y l√† solution ƒë·∫ßy ƒë·ªß, ƒë√°p ·ª©ng 100% y√™u c·∫ßu technical c·ªßa milestone project, v·ªõi focus v√†o **Apache Spark** v√† **Lambda Architecture**, demo ƒë∆∞·ª£c t·∫•t c·∫£ intermediate-level Spark skills y√™u c·∫ßu.

B·∫°n c√≥ mu·ªën t√¥i:

1. T·∫°o code chi ti·∫øt cho t·ª´ng component?
2. Setup Kubernetes manifests?
3. Vi·∫øt demo scripts?
4. T·∫°o presentation slides?
